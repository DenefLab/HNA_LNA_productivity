{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prubbens/anaconda3/lib/python3.4/site-packages/matplotlib/__init__.py:878: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "'''Import packages'''\n",
    "'''Requires numpy, pandas, scikit-learn, and matplotlib/seaborn'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "'''Import script which contains functions'''\n",
    "import analysis_functions\n",
    "from analysis_functions import get_r2\n",
    "from analysis_functions import get_lassoCV\n",
    "from analysis_functions import perform_randomizedLasso\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#If we want to time the implementation: \n",
    "#import time\n",
    "#start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataframes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Import data'''\n",
    "data_abs = pd.read_csv('data/Chloroplasts_removed/nochloro_absolute_otu.tsv', sep=' ', index_col=None, header=0)\n",
    "data_rel = pd.read_csv('data/Chloroplasts_removed/nochloro_relative_otu.tsv', sep=' ', index_col=None, header=0)\n",
    "target = pd.read_csv('data/Chloroplasts_removed/nochloro_HNA_LNA.tsv', sep=' ', index_col=0, header=0)\n",
    "productivity = pd.read_csv('data/Chloroplasts_removed/productivity_data.tsv', sep=' ', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Set sample names as index and shuffle data'''\n",
    "data_abs.set_index(target.loc[data_abs.index,'samples'],inplace=True)\n",
    "data_rel.set_index(target.loc[data_rel.index,'samples'],inplace=True)\n",
    "target.index = target.samples\n",
    "data_abs = data_abs.sample(frac=1, random_state=3)\n",
    "data_rel = data_rel.sample(frac=1, random_state=3)\n",
    "target = target.sample(frac=1, random_state=3)\n",
    "productivity = productivity.sample(frac=1, random_state=3)\n",
    "\n",
    "#Create target columns of HNA-values: \n",
    "hna = target.loc[:,'HNA.cells']\n",
    "hna_rel = hna/target.loc[:,'Total.cells']\n",
    "hna = pd.Series(hna, index=hna.index)\n",
    "hna_rel = pd.Series(hna_rel, index=hna.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- PREPROCESSING OF DATA --**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)**: filter out those OTUs which have very low abundances and so give rise to (almost) zero-columns. Therefore an OTU has to have a minimal relative abundance one, defined by the parameter $abun$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, depending on the problem set-up, I use a **second constraint** which states that an OTU must have a relative abundance > $abun$ in one of the **productivity** samples. In this way we're going to bias the OTU-selection towards the ones which are considerably present in the productivity samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Filtering based on productivity samples, not needed for first part of analysis'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Filtering based on productivity samples, not needed for first part of analysis'''\n",
    "#retain only productivity samples \n",
    "#productivity = productivity.dropna(subset=['tot_bacprod'])\n",
    "#remove high productivity samples (>90)\n",
    "#productivity = productivity[productivity.tot_bacprod < 90]\n",
    "\n",
    "#idx_prod = productivity.samples.values\n",
    "#display(idx_prod)\n",
    "#prod = pd.Series(productivity.tot_bacprod.values, index=idx_prod)\n",
    "#prod_error = pd.Series(productivity.SD_tot_bacprod.values, index=idx_prod)\n",
    "#prod_rel_error = prod_error/prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Parameter abun for initial filtering of OTUs'''\n",
    "abun = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OTUs: 263\n"
     ]
    }
   ],
   "source": [
    "from analysis_functions import preprocess_df\n",
    "data_abs = preprocess_df(data_abs,abun,True)\n",
    "otus = list(data_abs.columns)\n",
    "\n",
    "print('Number of OTUs: ' + str(len(otus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that this number is the same whether we use absolute or relative abundances, as the filtering is based on a minimal _relative_ abundance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some variables to store information and to create inner and outer CV-folds\n",
    "\n",
    "cv_out = 10\n",
    "cv_in = 5\n",
    "outer_cv = KFold(n_splits=cv_out, shuffle=False)\n",
    "\n",
    "otu_scores_cv = pd.DataFrame(columns=otus)\n",
    "r2_cv = np.zeros(cv_out)\n",
    "thresholds_cv = np.zeros(cv_out)\n",
    "\n",
    "pred = pd.Series(index=data_abs.index)\n",
    "final_scores = pd.DataFrame(columns=otus)\n",
    "\n",
    "thresholds = np.arange(0,1,0.02)\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the performance without using the randomized Lasso: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² based on absolute abundances: 0.886182335215\n",
      "R² based on relative abundances: 0.787474600975\n"
     ]
    }
   ],
   "source": [
    "from analysis_functions import perform_nested_lasso_cv\n",
    "\n",
    "alphas_abs, preds_abs = perform_nested_lasso_cv(data_abs[otus], hna, cv_out, cv_in)\n",
    "alphas_rel, preds_rel = perform_nested_lasso_cv(data_rel[otus], hna, cv_out, cv_in)\n",
    "\n",
    "r2_abs = get_r2(hna, preds_abs)\n",
    "r2_rel = get_r2(hna, preds_rel)\n",
    "\n",
    "print('R² based on absolute abundances: ' + str(r2_abs))\n",
    "print('R² based on relative abundances: ' + str(r2_rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ is already quite good. Let's check the $R^2$ if we would use the total cell counts to predict the HNA-counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² based on total cell counts: 0.711185406184\n"
     ]
    }
   ],
   "source": [
    "alphas_totcells, preds_totcells = perform_nested_lasso_cv(pd.DataFrame(target['Total.cells']), hna, cv_out, cv_in)\n",
    "r2_totcells = get_r2(hna, preds_totcells)\n",
    "\n",
    "print('R² based on total cell counts: ' + str(r2_totcells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we already see that there already is added value in using absolute cell counts to predict the HNA-counts:\n",
    "- $R^2_{HNA(abs)} = 0.768$\n",
    "- $R^2_{HNA(Total.cells)} = 0.712$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we use the **_Randomized Lasso_**: this method makes use of two kinds of randomization in order to select variables (i.e., OTU's) with a certain _stability_: (1) it fits a Lasso to various bootstrap subsamples and (2) it perturbs the initial weighting of certain variables. \n",
    "\n",
    "This results in a $score \\in [0,1]$ that is assigned to variables, with 0 denoting the case where a variable is never chosen by the Lasso, and 1 denoting the case where a variable always is chosen. In other words, the higher the score, the more important a variable can be considered to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a **10x5 nested cross-validation** scheme to evaluate our total pipeline: this means that the preprocessing and randomized Lasso is now included in this pipeline (in order to be sure not to overfit, motivated by the paragraph ''7.10.2 The Wrong and Right Way to Do Cross-validation\" in ESLII): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First goal: ** try to pinpoint those OTU's for which we are sure they are present in the '_HNA-cloud_'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx_train, idx_test in outer_cv.split(data_abs, hna):\n",
    "    lassoCV = get_lassoCV(cv_in)\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(data_abs.iloc[idx_train,:])\n",
    "    #data_abs = pd.DataFrame(scaler.transform(data_abs[otus]),index=data_abs.index,columns=otus)    \n",
    "    lassoCV.fit(data_abs.iloc[idx_train,:], hna[idx_train])\n",
    "    mse = np.sum(lassoCV.mse_path_, axis=1)\n",
    "    mse_min = np.min(mse)\n",
    "    alpha = lassoCV.alpha_\n",
    "    \n",
    "    otu_scores = pd.Series(perform_randomizedLasso(data_abs.iloc[idx_train,:], hna[idx_train], alpha), index=otus)\n",
    "    otu_scores_cv.loc[t] = otu_scores\n",
    "    otu_scores.sort_values(ascending=False,inplace=True)\n",
    "        \n",
    "    mse_scores = np.zeros(len(thresholds))\n",
    "    dummy=0\n",
    "        \n",
    "    scores = otu_scores\n",
    "    \n",
    "    for thr in thresholds: \n",
    "        scores = otu_scores[otu_scores.values > thr]\n",
    "        features_new = scores.index\n",
    "        if(len(features_new) > 0): \n",
    "            lassoCV = get_lassoCV(cv_in)\n",
    "            lassoCV.fit(data_abs.ix[idx_train,features_new],hna[idx_train])\n",
    "            #alphas, preds = perform_nested_ridge_cv(data_abs[features_new],hna) #We could use this if we want a different evaluation model\n",
    "            mse = np.sum(lassoCV.mse_path_, axis=1)\n",
    "            mse_scores[dummy] = np.min(mse)\n",
    "        dummy+=1\n",
    "        \n",
    "    mse_scores = mse_scores[np.nonzero(mse_scores)]\n",
    "    #mse_min = mse_scores.min()\n",
    "    mse_min_idx = mse_scores.argmin()\n",
    "    thresh_max = thresholds[mse_min_idx]\n",
    "    thresholds_cv[t] = thresh_max\n",
    "    optimal_scores = otu_scores[otu_scores.values>thresh_max]\n",
    "    selected_otus = optimal_scores.index\n",
    "    \n",
    "    lassoCV = get_lassoCV(cv_in)\n",
    "    lassoCV.fit(data_abs.ix[idx_train, selected_otus], hna[idx_train])\n",
    "    alpha = lassoCV.alpha_\n",
    "    lasso = Lasso(alpha,max_iter=20000,normalize=True)\n",
    "    pred_cv_final = cross_val_predict(lasso, data_abs.ix[idx_train, selected_otus], hna[idx_train], cv=cv_in)\n",
    "    r2_cv[t] = get_r2(pred_cv_final, hna[idx_train])\n",
    "    pred.iloc[idx_test] = lassoCV.predict(data_abs.ix[idx_test,selected_otus])\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(R²_cv), std(R²_cv): 0.948683335443, 0.0134946321203\n",
      "R²_test: 0.905700250698\n"
     ]
    }
   ],
   "source": [
    "r2_final = get_r2(pred,hna)   \n",
    "print('mean(R²_cv), std(R²_cv): ' + str(r2_cv.mean()) + ', ' +  str(r2_cv.std()))\n",
    "print('R²_test: ' + str(r2_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that: \n",
    "- $R^2_{\\text{test}} = 0.906$ with normalization (this becomes 0.86 after standardization (not sure why yet)); \n",
    "- $\\bar{R^2}_{\\text{CV}} = 0.949 \\pm 0.013$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that we can now have a look to which extent OTU's are chosen in the various folds according to the randomized Lasso. In other words, we can take their average score, and only look at those OTU's which have a score higher than twice times the standard deviation from the average score (I have to check the following comment, but I think this means that with 95% certainty these OTU's are chosen according to the randomized Lasso); in other words, this means that the ones ranked in the top really are the most important ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Otu000098    0.693000\n",
       "Otu000005    0.651333\n",
       "Otu000025    0.550667\n",
       "Otu000123    0.544333\n",
       "Otu000124    0.523667\n",
       "Otu000011    0.517000\n",
       "Otu000057    0.517000\n",
       "Otu000009    0.515333\n",
       "Otu000050    0.504333\n",
       "Otu000047    0.503000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_otu_scores = otu_scores_cv.mean()\n",
    "std_otu_scores = otu_scores_cv.std()\n",
    "mean_otu_scores.sort_values(ascending=False,inplace=True)\n",
    "avg_thresh = thresholds_cv.mean()\n",
    "std_thresh = thresholds_cv.std()\n",
    "otu_scores_cv.to_csv('HNA_scores_OTU_stand_10x5CV_abun0.01.csv')\n",
    "otus_final = mean_otu_scores[mean_otu_scores > (avg_thresh+2*std_thresh)]\n",
    "display(otus_final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- WHAT IF WE ONLY CONSIDER THOSE OTU's WHICH ARE SIGNIFICANTLY PRESENT IN THE PRODUCTIVITY SAMPLES? --**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Import data'''\n",
    "data_abs = pd.read_csv('data/Chloroplasts_removed/nochloro_absolute_otu.tsv', sep=' ', index_col=None, header=0)\n",
    "data_rel = pd.read_csv('data/Chloroplasts_removed/nochloro_relative_otu.tsv', sep=' ', index_col=None, header=0)\n",
    "target = pd.read_csv('data/Chloroplasts_removed/nochloro_HNA_LNA.tsv', sep=' ', index_col=0, header=0)\n",
    "productivity = pd.read_csv('data/Chloroplasts_removed/productivity_data.tsv', sep=' ', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Set sample names as index and shuffle data'''\n",
    "data_abs.set_index(target.samples,inplace=True)\n",
    "data_rel.set_index(target.samples,inplace=True)\n",
    "data_abs = data_abs.sample(frac=1, random_state=3)\n",
    "data_rel = data_rel.sample(frac=1, random_state=3)\n",
    "target = target.sample(frac=1, random_state=3)\n",
    "productivity = productivity.sample(frac=1, random_state=3)\n",
    "\n",
    "#Create target columns of HNA-values: \n",
    "hna = target.loc[:,'HNA.cells']\n",
    "hna_rel = hna/target.loc[:,'Total.cells']\n",
    "hna = pd.Series(hna, index=hna.index)\n",
    "hna_rel = pd.Series(hna_rel, index=hna_rel.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing: ** First filter productivity outliers (productivity > 90). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retain only productivity samples \n",
    "productivity = productivity.dropna(subset=['tot_bacprod'])\n",
    "#remove high productivity samples (>90)\n",
    "productivity = productivity[productivity.tot_bacprod < 90]\n",
    "\n",
    "idx_prod = productivity.samples.values\n",
    "#display(idx_prod)\n",
    "prod = pd.Series(productivity.tot_bacprod.values, index=idx_prod)\n",
    "#prod_error = pd.Series(productivity.SD_tot_bacprod.values, index=idx_prod)\n",
    "#prod_rel_error = prod_error/prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Check different abun '''\n",
    "abun = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OTUs: 121\n"
     ]
    }
   ],
   "source": [
    "from analysis_functions import preprocess_df\n",
    "data_abs_prod = data_abs.loc[idx_prod,:] \n",
    "data_abs_prod = preprocess_df(data_abs_prod,abun,True)\n",
    "otus_prod = list(data_abs_prod.columns)\n",
    "\n",
    "print('Number of OTUs: ' + str(len(otus_prod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² based on absolute abundances after productivity filtering: 0.852836976091\n"
     ]
    }
   ],
   "source": [
    "alphas_abs, preds_abs = perform_nested_lasso_cv(data_abs[otus_prod], hna, cv_out, cv_in)\n",
    "r2_abs = get_r2(hna, preds_abs)\n",
    "print('R² based on absolute abundances after productivity filtering: ' + str(r2_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=0\n",
    "\n",
    "for idx_train, idx_test in outer_cv.split(data_abs, hna):\n",
    "    lassoCV = get_lassoCV(5)\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(data_abs.iloc[idx_train,:])\n",
    "    #data_abs = pd.DataFrame(scaler.transform(data_abs[otus]),index=data_abs.index,columns=otus)    \n",
    "    lassoCV.fit(data_abs.ix[idx_train,otus_prod], hna[idx_train])\n",
    "    mse = np.sum(lassoCV.mse_path_, axis=1)\n",
    "    mse_min = np.min(mse)\n",
    "    alpha = lassoCV.alpha_\n",
    "    \n",
    "    otu_prod_scores = pd.Series(perform_randomizedLasso(data_abs.ix[idx_train,otus_prod], hna[idx_train], alpha), index=otus_prod)\n",
    "    otu_scores_cv.loc[t] = otu_prod_scores\n",
    "    otu_prod_scores.sort_values(ascending=False,inplace=True)\n",
    "        \n",
    "    mse_scores = np.zeros(len(thresholds))\n",
    "    dummy=0\n",
    "        \n",
    "    scores = otu_prod_scores\n",
    "    \n",
    "    for thr in thresholds: \n",
    "        scores = otu_prod_scores[otu_prod_scores.values > thr]\n",
    "        features_new = scores.index\n",
    "        if(len(features_new) > 0): \n",
    "            lassoCV = get_lassoCV(cv_in)\n",
    "            lassoCV.fit(data_abs.ix[idx_train,features_new],hna[idx_train])\n",
    "            #alphas, preds = perform_nested_ridge_cv(data_abs[features_new],hna) #We could use this if we want a different evaluation model\n",
    "            mse = np.sum(lassoCV.mse_path_, axis=1)\n",
    "            mse_scores[dummy] = np.min(mse)\n",
    "        dummy+=1\n",
    "        \n",
    "    mse_scores = mse_scores[np.nonzero(mse_scores)]\n",
    "    #mse_min = mse_scores.min()\n",
    "    mse_min_idx = mse_scores.argmin()\n",
    "    thresh_max = thresholds[mse_min_idx]\n",
    "    thresholds_cv[t] = thresh_max\n",
    "    optimal_scores = otu_prod_scores[otu_prod_scores.values>thresh_max]\n",
    "    selected_otus = optimal_scores.index\n",
    "    \n",
    "    lassoCV = get_lassoCV(cv_in)\n",
    "    lassoCV.fit(data_abs.ix[idx_train, selected_otus], hna[idx_train])\n",
    "    alpha = lassoCV.alpha_\n",
    "    lasso = Lasso(alpha,max_iter=20000,normalize=True)\n",
    "    pred_cv_final = cross_val_predict(lasso, data_abs.ix[idx_train, selected_otus], hna[idx_train], cv=cv_in)\n",
    "    r2_cv[t] = get_r2(pred_cv_final, hna[idx_train])\n",
    "    pred.iloc[idx_test] = lassoCV.predict(data_abs.ix[idx_test,selected_otus])\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(R²_cv), std(R²_cv): 0.895356559591, 0.0146182009881\n",
      "R²_test: 0.844020313234\n"
     ]
    }
   ],
   "source": [
    "r2_final = get_r2(pred,hna)   \n",
    "print('mean(R²_cv), std(R²_cv): ' + str(r2_cv.mean()) + ', ' +  str(r2_cv.std()))\n",
    "print('R²_test: ' + str(r2_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Otu000027    0.962121\n",
       "Otu000057    0.961515\n",
       "Otu000005    0.928788\n",
       "Otu000109    0.908333\n",
       "Otu000067    0.858485\n",
       "Otu000123    0.857333\n",
       "Otu000058    0.855455\n",
       "Otu000043    0.848333\n",
       "Otu000050    0.804848\n",
       "Otu000048    0.675152\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_otu_scores = otu_scores_cv.mean()\n",
    "std_otu_scores = otu_scores_cv.std()\n",
    "mean_otu_scores.sort_values(ascending=False,inplace=True)\n",
    "avg_thresh = thresholds_cv.mean()\n",
    "std_thresh = thresholds_cv.std()\n",
    "otu_scores_cv.to_csv('HNA_scores_prodfiltering_OTU_stand_10x5CV_abun0.01.csv')\n",
    "otus_final = mean_otu_scores[mean_otu_scores > (avg_thresh)]#+std_thresh)]\n",
    "display(otus_final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- SUMMARY --**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are able to predict the changes in the HNA-cloud up to high accuracy $R^2_{HNA(final)} = 0.953$, using our pipeline; This gives us a reduced set of OTU's (340 $\\rightarrow$ 100). \n",
    "- This score is higher than using a 'standard' Lasso on all OTU's after initial filtering ($R^2_{HNA(abs)} = 0.768$). \n",
    "- Our pipeline in which we use absolute abundances gives added value, as only using the total cell counts gives us an $R^2_{HNA(Total.cells)} = 0.712$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- SUMMARY (Part 2) --**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we only include those OTU's which are significantly present in the productivity samples, the results change quite a bit. \n",
    "- $R^2_{final} = 0.897$, however the reduced set of OTU's becomes 95 $\\rightarrow$ 26.\n",
    "- This means that we can narrow down our subset of OTU's to 26 if we want to consider those OTU's which are significantly present in the productivity samples and can be related to the HNA-cloud. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
